{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset  # for data-sets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "len_data 11000\n"
     ]
    }
   ],
   "source": [
    "# get all the image and mask\n",
    "\n",
    "folder_data = np.load(\"all_slices_image_1channel.npy\")\n",
    "folder_mask = np.load(\"all_slices_mask_1channel.npy\")\n",
    "print(type(folder_data))\n",
    "\n",
    "# split the data into train and test\n",
    "len_data = len(folder_data)\n",
    "print('len_data',len_data)\n",
    "train_size = 0.8\n",
    "\n",
    "#move the slices into ararys for train(img+msk) and test(img+msk)\n",
    "train_image_npy = folder_data[:int(len_data*train_size)]\n",
    "train_mask_npy = folder_mask[:int(len_data*train_size)]\n",
    "\n",
    "validation_image_npy = folder_data[int(len_data*train_size):]\n",
    "validation_mask_npy = folder_mask[int(len_data*train_size):]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_npy, mask_npy, train=True):   # initial logic happens like transform\n",
    "        \n",
    "        self.image_npy = image_npy\n",
    "        self.mask_npy = mask_npy\n",
    "        self.transforms  = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "       # print('Index',index)\n",
    "        self.image = np.asarray(self.image_npy[index])\n",
    "        self.mask = np.asarray(self.mask_npy[index])\n",
    "      #  print('Before converting into tensor image and mask shape', self.image.shape ,  self.image.shape)\n",
    "        self.image = self.transforms(self.image)\n",
    "        self.mask = self.transforms(self.mask)\n",
    "       # print('After Image and Mask Shape',self.image.shape, self.mask.shape)\n",
    "        return self.image, self.mask\n",
    "\n",
    "    def __len__(self):  # return count of sample we have\n",
    "\n",
    "        return len(self.image_npy)\n",
    "\n",
    "batch_size = 5\n",
    "train_dataset = CustomDataset(train_image_npy, train_mask_npy, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "validation_dataset = CustomDataset(validation_image_npy, validation_mask_npy, train=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': validation_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8800"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Image torch.Size([1, 256, 256])\n",
      "Mask torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset[8799]))\n",
    "print('Image',train_dataset[8799][0].shape)\n",
    "print('Mask', train_dataset[8799][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, masks = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import pytorch_unet\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from model import UNet\n",
    "model = UNet().to(device)\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from dice_loss import dice_coeff\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):  \n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target) \n",
    "    dice = dice_coeff(pred, target)\n",
    "  #  print(bce)\n",
    "    #print('bce', type(bce))       \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "   # print('dice', type(dice))\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "   # metrics['loss'] += loss.data.cpu().numpy() * target.size(0)   \n",
    "    return bce\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {}\".format(str(k), str(metrics[k] / epoch_samples)))\n",
    "    \n",
    "    print( (\" {}: {} \").format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    best_dice = 0.5\n",
    "       \n",
    "    for epoch in range(num_epochs): #25\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)    \n",
    "        since = time.time()     \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])                    \n",
    "                model.train()  # Set model to training mode\n",
    "                \n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    if phase == 'train':\n",
    "                        i=0\n",
    "                    else:\n",
    "                        i=2\n",
    "                    loss = calc_loss(outputs, labels, metrics)                  \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['bce'] / epoch_samples\n",
    "            epoch_dice = metrics['dice'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and best_dice < epoch_dice:\n",
    "                print(\"saving best model\")\n",
    "                best_dice = epoch_dice\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_dice))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/29\n",
      "----------\n",
      "LR 0.0001\n",
      " train: bce: 0.6971829482777552, dice: [0.01023416] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 44s\n",
      "Epoch 1/29\n",
      "----------\n",
      "LR 0.0001\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 3/29\n",
      "----------\n",
      "LR 0.0001\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 4/29\n",
      "----------\n",
      "LR 0.0001\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 5/29\n",
      "----------\n",
      "LR 1e-05\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 6/29\n",
      "----------\n",
      "LR 1e-05\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 7/29\n",
      "----------\n",
      "LR 1e-05\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 8/29\n",
      "----------\n",
      "LR 1e-05\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 9/29\n",
      "----------\n",
      "LR 1e-05\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 10/29\n",
      "----------\n",
      "LR 1.0000000000000002e-06\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 11/29\n",
      "----------\n",
      "LR 1.0000000000000002e-06\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 12/29\n",
      "----------\n",
      "LR 1.0000000000000002e-06\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 13/29\n",
      "----------\n",
      "LR 1.0000000000000002e-06\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 14/29\n",
      "----------\n",
      "LR 1.0000000000000002e-06\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 15/29\n",
      "----------\n",
      "LR 1.0000000000000002e-07\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 16/29\n",
      "----------\n",
      "LR 1.0000000000000002e-07\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 48s\n",
      "Epoch 17/29\n",
      "----------\n",
      "LR 1.0000000000000002e-07\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 48s\n",
      "Epoch 18/29\n",
      "----------\n",
      "LR 1.0000000000000002e-07\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 19/29\n",
      "----------\n",
      "LR 1.0000000000000002e-07\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 20/29\n",
      "----------\n",
      "LR 1.0000000000000002e-08\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 21/29\n",
      "----------\n",
      "LR 1.0000000000000002e-08\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 22/29\n",
      "----------\n",
      "LR 1.0000000000000002e-08\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 47s\n",
      "Epoch 23/29\n",
      "----------\n",
      "LR 1.0000000000000002e-08\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 25/29\n",
      "----------\n",
      "LR 1.0000000000000003e-09\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 46s\n",
      "Epoch 26/29\n",
      "----------\n",
      "LR 1.0000000000000003e-09\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 47s\n",
      "Epoch 27/29\n",
      "----------\n",
      "LR 1.0000000000000003e-09\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 47s\n",
      "Epoch 28/29\n",
      "----------\n",
      "LR 1.0000000000000003e-09\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 47s\n",
      "Epoch 29/29\n",
      "----------\n",
      "LR 1.0000000000000003e-09\n",
      " train: bce: 0.6931471824645996, dice: [5.7182223e-09] \n",
      " val: bce: 0.6931471824645996, dice: [5.721024e-09] \n",
      "1m 47s\n",
      "Best val loss: 0.500000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 2\n",
    "\n",
    "from model import UNet\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "#opt = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.0005)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma= 0.1)\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
